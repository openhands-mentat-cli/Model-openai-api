# Core dependencies for llama.cpp with optimizations
llama-cpp-python[server]==0.2.36
fastapi==0.110.0
uvicorn[standard]==0.27.1
pydantic==2.6.1
requests==2.31.0
huggingface-hub==0.20.3
